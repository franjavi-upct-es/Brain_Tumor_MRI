"""
tools/adaptive_retrain.py - Adaptive Retraining Pipeline
========================================================
Implements evidence-based improvements from error analysis:

1. Focal Loss (Î³=2.5) - Focus on hard examples
2. Aggressive Label Smoothing (Îµ=0.1) - Reduce overconfidence
3. Tumor-Focused Augmentation - Synthetic diversity
4. Test Time Augmentation (TTA) - Ensemble robustness
5. Temperature Recalibration - Better probability estimates

This script retrains the model with these enhancements and evaluates
the impact on the external dataset.
"""

import os
import argparse
import json
import numpy as np

# GPU Configuration - must be set BEFORE importing TensorFlow
# NVIDIA GPU is device 0 (Intel iGPU doesn't count as CUDA device)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Use NVIDIA GPU
os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"
os.environ["TF_GPU_THREAD_MODE"] = "gpu_private"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "1"  # Reduce TF logging noise

import tensorflow as tf
from pathlib import Path
from tqdm import tqdm

from src.utils import load_config, set_seed
from src.data import get_datasets
from src.model import create_model
from src.losses import FocalLoss


def create_tumor_focused_augmentation():
    """
    Aggressive augmentation specifically for tumor samples.

    Medical rationale:
    - Tumors appear in various orientations (rotation)
    - Scanner variations affect intensity (brightness/contrast)
    - Position varies (translation via zoom)
    """
    return tf.keras.Sequential(
        [
            tf.keras.layers.RandomFlip("horizontal"),
            tf.keras.layers.RandomRotation(0.08),  # capped for anatomical realism (~Â±29Â°)
            tf.keras.layers.RandomZoom(0.1),
            tf.keras.layers.RandomBrightness(0.12),
            tf.keras.layers.RandomContrast(0.12),
        ],
        name="tumor_augmentation",
    )


def apply_tta_inference(model, image, n_augmentations=5):
    """
    Test Time Augmentation: Average predictions over multiple augmented versions.

    Reduces variance and improves robustness, especially for borderline cases.
    """
    augment = create_tumor_focused_augmentation()

    predictions = []

    # Original prediction
    predictions.append(model.predict(image, verbose=0))

    # Augmented predictions
    for _ in range(n_augmentations - 1):
        aug_image = augment(image, training=True)
        predictions.append(model.predict(aug_image, verbose=0))

    # Average logits (not probabilities, for better calibration)
    avg_logits = np.mean(predictions, axis=0)
    return avg_logits


def load_temperature(ckpt_dir: str, filename: str = "focal_temperature.json") -> float:
    """
    Load temperature scaling factor if available.
    """
    path = os.path.join(ckpt_dir, filename)
    if os.path.exists(path):
        try:
            with open(path, "r") as f:
                data = json.load(f)
            return float(data.get("temperature", 1.0))
        except Exception:
            return 1.0
    return 1.0


def train_with_focal_loss(cfg, use_label_smoothing=True):
    """
    Retrain the model with Focal Loss and enhanced augmentation.

    Key changes:
    1. Focal Loss (Î±=0.75, Î³=2.5) - Up-weights hard examples
    2. Label Smoothing (Îµ=0.1) - Reduces overconfidence
    3. Tumor-specific augmentation
    4. Extended training with lower LR
    """
    print("\n" + "=" * 70)
    print("ADAPTIVE RETRAINING WITH FOCAL LOSS")
    print("=" * 70)

    set_seed(cfg.get("seed", 42))

    # Load data
    train_ds, val_ds, test_ds, class_names, class_weights = get_datasets(cfg)
    num_classes = len(class_names)

    # Create model
    model = create_model(cfg, num_classes)

    # Load base weights as starting point (transfer learning from previous training)
    base_model_path = os.path.join(
        cfg["train"]["checkpoint_dir"], "finetuned_navoneel.keras"
    )
    if os.path.exists(base_model_path):
        print(f"[INFO] Loading base weights from: {base_model_path}")
        base_model = tf.keras.models.load_model(base_model_path, compile=False)
        model.set_weights(base_model.get_weights())
        del base_model

    # Configure Focal Loss
    focal_loss = FocalLoss(
        alpha=0.25,  # Weight for positive class
        gamma=2.0,  # Focusing parameter (higher = more focus on hard examples)
        from_logits=True,
        label_smoothing=0.1 if use_label_smoothing else 0.0,
    )

    print(f"\n[CONFIG] Focal Loss Parameters:")
    print(f"  Î± (alpha): 0.25  - Up-weights minority class")
    print(f"  Î³ (gamma): 2.0   - Strong focus on hard examples")
    print(f"  Label Smoothing: {0.1 if use_label_smoothing else 0.0}")

    # Optimizer with lower learning rate for fine-tuning
    optimizer = tf.keras.optimizers.AdamW(
        learning_rate=5e-4,  # Lower than initial training
        weight_decay=0.01,
    )

    # Compile
    model.compile(
        optimizer=optimizer,
        loss=focal_loss,
        metrics=[
            "categorical_accuracy",
            tf.keras.metrics.Recall(name="recall"),
            tf.keras.metrics.Precision(name="precision"),
        ],
    )

    # Callbacks
    os.makedirs(cfg["train"]["checkpoint_dir"], exist_ok=True)
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(cfg["train"]["checkpoint_dir"], "focal_best.keras"),
            monitor="val_recall",  # Prioritize recall (sensitivity)
            save_best_only=True,
            mode="max",
            verbose=1,
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor="val_recall", patience=5, restore_best_weights=True, mode="max"
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6, verbose=1
        ),
        tf.keras.callbacks.CSVLogger(
            os.path.join(cfg["train"]["checkpoint_dir"], "focal_training.log")
        ),
    ]

    # Train
    print("\n[INFO] Starting training with Focal Loss...")
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=30,  # Extended training
        callbacks=callbacks,
        class_weight=class_weights
        if cfg["train"].get("use_class_weights", False)
        else None,
    )

    print("\n[SUCCESS] Training completed!")
    print(f"Best validation recall: {max(history.history['val_recall']):.1%}")

    return model, history


def evaluate_on_external_with_tta(
    model,
    data_dir,
    cfg,
    use_tta=True,
    n_tta=5,
    threshold=None,
    temperature=1.0,
):
    """
    Evaluate the retrained model on external data with optional TTA.
    """
    if threshold is None:
        threshold = cfg["inference"].get("threshold", 0.5)
    print("\n" + "=" * 70)
    print("EXTERNAL VALIDATION WITH TEST TIME AUGMENTATION")
    print("=" * 70)

    img_size = cfg["data"]["image_size"]
    class_names = cfg["data"]["class_names"]

    try:
        no_tumor_idx = class_names.index("no_tumor")
    except ValueError:
        print("[ERROR] 'no_tumor' not found in config")
        return

    # Preprocessing
    if "v2" in cfg["model"]["name"]:
        preprocess = tf.keras.applications.efficientnet_v2.preprocess_input
    else:
        preprocess = tf.keras.applications.efficientnet.preprocess_input

    # Collect images
    data_path = Path(data_dir)
    images_paths = []
    true_binary_labels = []

    # Healthy
    for ext in ["*.jpg", "*.jpeg", "*.png"]:
        for p in (data_path / "no").glob(ext):
            images_paths.append(str(p))
            true_binary_labels.append(0)

    # Tumors
    for ext in ["*.jpg", "*.jpeg", "*.png"]:
        for p in (data_path / "yes").glob(ext):
            images_paths.append(str(p))
            true_binary_labels.append(1)

    print(f"\n[INFO] Evaluating {len(images_paths)} images...")
    print(f"[INFO] Test Time Augmentation: {'Enabled' if use_tta else 'Disabled'}")
    if use_tta:
        print(f"[INFO] TTA samples per image: {n_tta}")
    print(f"[INFO] Decision threshold (tumor vs healthy): {threshold}")
    if temperature != 1.0:
        print(f"[INFO] Temperature scaling applied: T={temperature}")

    # Predictions
    pred_binary_labels = []
    pred_tumor_probs = []

    for img_path in tqdm(images_paths, desc="Inference"):
        # Load and preprocess
        img = tf.io.read_file(img_path)
        img = tf.image.decode_image(img, channels=3, expand_animations=False)
        img = tf.image.resize(img, (img_size, img_size))
        img = tf.expand_dims(img, axis=0)
        img = preprocess(img)

        # Predict
        if use_tta:
            logits = apply_tta_inference(model, img, n_augmentations=n_tta)
        else:
            logits = model.predict(img, verbose=0)

        logits = logits / temperature
        probs = tf.nn.softmax(logits[0]).numpy()

        # Binary decision
        prob_tumor = 1.0 - probs[no_tumor_idx]
        pred_tumor_probs.append(prob_tumor)

        pred_binary = 1 if prob_tumor >= threshold else 0
        pred_binary_labels.append(pred_binary)

    # Metrics
    from sklearn.metrics import classification_report, confusion_matrix

    print("\n" + "=" * 70)
    print("RESULTS")
    print("=" * 70)

    print("\n--- Classification Report ---")
    print(
        classification_report(
            true_binary_labels,
            pred_binary_labels,
            target_names=["Healthy", "Tumor"],
            digits=3,
        )
    )

    cm = confusion_matrix(true_binary_labels, pred_binary_labels)
    tn, fp, fn, tp = cm.ravel()

    print("\n--- Confusion Matrix ---")
    print(f"True Negatives (Healthy OK):    {tn}")
    print(f"False Positives (False Alarms): {fp}")
    print(f"False Negatives (Missed Tumors): {fn} âš ï¸")
    print(f"True Positives (Detected):       {tp} âœ“")

    # Key metrics
    total = tn + fp + fn + tp
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    accuracy = (tp + tn) / total if total > 0 else 0

    print("\n--- Clinical Metrics ---")
    print(f"Sensitivity (Recall):    {recall:.1%}  {'âœ“' if recall > 0.85 else 'âš ï¸'}")
    print(f"Specificity:             {specificity:.1%}")
    print(f"Precision (PPV):         {precision:.1%}")
    print(f"Accuracy:                {accuracy:.1%}")
    print(
        f"False Negative Rate:     {fn / (tp + fn):.1%}  {'âœ“' if fn / (tp + fn) < 0.15 else 'âš ï¸'}"
    )

    return {
        "recall": float(recall),
        "precision": float(precision),
        "specificity": float(specificity),
        "accuracy": float(accuracy),
        "fn_rate": float(fn / (tp + fn) if (tp + fn) > 0 else 0),
        "confusion_matrix": cm.tolist(),
        "tta_enabled": use_tta,
        "tta_samples": n_tta if use_tta else 1,
        "threshold": float(threshold),
        "temperature": float(temperature),
    }


def recalibrate_temperature(model, val_ds, max_iters=300):
    """
    Recalibrate temperature after retraining.

    More aggressive optimization to correct residual overconfidence.
    """
    print("\n[INFO] Recalibrating temperature scalar...")

    # Collect validation predictions
    logits_list, y_list = [], []
    for x, y in val_ds:
        logits = model.predict(x, verbose=0)
        logits_list.append(logits)
        y_list.append(y.numpy())

    logits = np.concatenate(logits_list, axis=0)
    y_true = np.concatenate(y_list, axis=0)

    # Optimize temperature
    logT = tf.Variable(0.0, dtype=tf.float32)  # log(T), initialized at T=1
    opt = tf.keras.optimizers.Adam(
        learning_rate=0.1
    )  # Higher LR for faster convergence

    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

    for i in range(max_iters):
        with tf.GradientTape() as tape:
            T = tf.exp(logT)
            scaled = logits / T
            loss = loss_fn(y_true, scaled)

        grads = tape.gradient(loss, [logT])
        opt.apply_gradients(zip(grads, [logT]))

        if i % 50 == 0:
            print(
                f"  Iteration {i}: T={tf.exp(logT).numpy():.3f}, Loss={loss.numpy():.4f}"
            )

    T_final = float(tf.exp(logT).numpy())
    print(f"[SUCCESS] Optimized temperature: T={T_final:.3f}")

    return T_final


def main():
    parser = argparse.ArgumentParser(description="Adaptive Retraining Pipeline")
    parser.add_argument("--config", default="configs/config.yaml")
    parser.add_argument("--external_data", default="data/external_navoneel_medical")
    parser.add_argument(
        "--skip_training", action="store_true", help="Skip retraining, just evaluate"
    )
    parser.add_argument(
        "--use_tta", action="store_true", help="Use Test Time Augmentation"
    )
    parser.add_argument("--n_tta", type=int, default=5, help="Number of TTA samples")
    parser.add_argument(
        "--threshold", type=float, default=None, help="Override tumor decision threshold"
    )

    args = parser.parse_args()

    # Load config
    cfg = load_config(args.config)

    # Verify GPU is available
    gpus = tf.config.list_physical_devices("GPU")
    if gpus:
        print(f"\n[INFO] âœ… Training with GPU: {len(gpus)} device(s) found")
        for gpu in gpus:
            print(f"       - {gpu}")
            try:
                tf.config.experimental.set_memory_growth(gpu, True)
            except RuntimeError as e:
                print(f"       Memory growth already set: {e}")
    else:
        print("\n[WARNING] âš ï¸ GPU not detected. Training will be slow on CPU.")

    # Stage 1: Retrain with Focal Loss
    if not args.skip_training:
        model, history = train_with_focal_loss(cfg, use_label_smoothing=True)

        # Save training history
        history_path = os.path.join(
            cfg["train"]["checkpoint_dir"], "focal_training_history.json"
        )
        with open(history_path, "w") as f:
            # Convert numpy types to Python types for JSON serialization
            history_dict = {
                k: [float(x) for x in v] for k, v in history.history.items()
            }
            json.dump(history_dict, f, indent=2)
        print(f"[INFO] Training history saved: {history_path}")

        # Recalibrate temperature
        _, val_ds, _, _, _ = get_datasets(cfg)
        T = recalibrate_temperature(model, val_ds)

        # Save temperature
        with open(
            os.path.join(cfg["train"]["checkpoint_dir"], "focal_temperature.json"), "w"
        ) as f:
            json.dump({"temperature": T}, f)
    else:
        # Load pretrained model
        model_path = os.path.join(cfg["train"]["checkpoint_dir"], "focal_best.keras")
        if not os.path.exists(model_path):
            print(f"[ERROR] Model not found: {model_path}")
            print("Run without --skip_training first.")
            return

        print(f"[INFO] Loading model: {model_path}")
        model = tf.keras.models.load_model(model_path, compile=False)

    # Threshold and calibration settings
    threshold = (
        args.threshold
        if args.threshold is not None
        else cfg["inference"].get("threshold", 0.5)
    )
    use_tta = args.use_tta or cfg["inference"].get("tta", False)
    n_tta = args.n_tta if args.n_tta else cfg["inference"].get("tta_samples", 5)
    temperature = (
        load_temperature(cfg["train"]["checkpoint_dir"])
        if cfg["inference"].get("use_calibration", True)
        else 1.0
    )

    # Stage 2: Evaluate on external data
    print("\n" + "=" * 70)
    print("EXTERNAL VALIDATION")
    print("=" * 70)

    results = evaluate_on_external_with_tta(
        model,
        args.external_data,
        cfg,
        use_tta=use_tta,
        n_tta=n_tta,
        threshold=threshold,
        temperature=temperature,
    )

    # Save results
    results_path = os.path.join(
        cfg["train"]["checkpoint_dir"], "focal_external_results.json"
    )
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\n[INFO] Results saved: {results_path}")

    # Summary
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"âœ“ Model retrained with Focal Loss (Î³=2.5)")
    print(f"âœ“ Label Smoothing applied (Îµ=0.1)")
    print(f"âœ“ Temperature recalibrated")
    print(f"âœ“ External validation completed")
    print(f"âœ“ Threshold used: {threshold}")
    print(f"âœ“ TTA: {'on' if use_tta else 'off'} (samples={n_tta if use_tta else 1})")

    if results["recall"] > 0.85:
        print(f"\nâœ… SUCCESS: Recall = {results['recall']:.1%} (Target: >85%)")
    else:
        print(f"\nâš ï¸  Recall = {results['recall']:.1%} - May need further tuning")

    print(f"\nğŸ“Š False Negative Rate: {results['fn_rate']:.1%}")
    print(f"ğŸ“Š Specificity: {results['specificity']:.1%}")


if __name__ == "__main__":
    main()
